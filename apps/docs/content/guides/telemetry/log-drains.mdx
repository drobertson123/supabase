---
id: 'log-drains'
title: 'Log Drains'
description: 'Getting started with Supabase Log Drains'
---

Log drains send all logs of the Supabase stack to one or more desired destinations. Log drains is only available for customers on Team and Enterprise Plans and is available in the dashboard under [**Project Settings** > **Log Drains**](/dashboard/project/_/settings/log-drains).

<Admonition type="note">

You can read about the initial announcement [here](/blog/log-drains) and vote for your preferred drains in [this discussion](https://github.com/orgs/supabase/discussions/28324?sort=top).

</Admonition>

## Supported destinations

The following table lists the supported destinations and the required setup configuration:

| Destination           | Transport Method | Configuration                                     |
| --------------------- | ---------------- | ------------------------------------------------- |
| Generic HTTP endpoint | HTTP             | URL <br /> HTTP Version <br/> Gzip <br /> Headers |
| Datadog               | HTTP             | API Key <br /> Region                             |
| Loki                  | HTTP             | URL <br /> Headers                                |

Supabase batces HTTP requests with a max of 250 logs or 1 second intervals, whichever happens first and compresses logs via Gzip if the destination supports it.

## Generic HTTP endpoint

Supabase sends logs as a POST request with a JSON body and supports HTTP/1 and HTTP/2 protocols.
You can configure custom headers for all requests.

<Admonition type="tip">

Requests to HTTP endpoints are **unsigned**. This is a temporary limitation and all requests will be signed in the near future.

</Admonition>

<Accordion
  type="default"
  openBehaviour="multiple"
>
    <AccordionItem
      header="Edge Function Uncompressed Example"
      id="uncompressed"
    >

1. Create and deploy the edge function

  Generate a new edge function template and update it to log out the received JSON payload. For simplicity, it accepts any request with an Anon Key.

  ```bash
  supabase functions new hello-world
  ```

  You can use this example snippet as an illustration of the received request.

  ```ts
  import 'npm:@supabase/functions-js/edge-runtime.d.ts'

  Deno.serve(async (req) => {
    const data = await req.json()

    console.log(`Received ${data.length} logs, first log:\n ${JSON.stringify(data[0])}`)
    return new Response(JSON.stringify({ message: 'ok' }), {
      headers: { 'Content-Type': 'application/json' },
    })
  })
  ```

  And then deploy it with:

  ```bash
  supabase functions deploy hello-world --project-ref [PROJECT REF]
  ```

  <Admonition type="caution">

  This creates an infinite loop, as the code generates an additional log event that eventually trigger a new request to this edge function. However, due to the batching nature of how Log Drain events are dispatched, the rate of edge function triggers will not increase greatly and will have an upper bound.

  </Admonition>

2. Configure the HTTP Drain

  Create a HTTP drain under the [**Project Settings** > **Log Drains**](/dashboard/project/_/settings/log-drains).

  - Disable Gzip, to receive the payload without compression.
  - Under URL, set it to your edge function URL `https://[PROJECT REF].supabase.co/functions/v1/hello-world`
  - Under Headers, set the `Authorization: Bearer [ANON KEY]`

<ProjectConfigVariables variable="publishableKey" />

    </AccordionItem>

    <AccordionItem
      header="Edge Function Gzip Example"
      id="gzip"
    >

Native JavaScript APIs can decompress Gzip payloads using `gunzipSync`. Read [the Edge Function compression guide](/docs/guides/functions/compression) for more information.

```ts
import { gunzipSync } from 'node:zlib'

Deno.serve(async (req) => {
  try {
    // Check if the request body is gzip compressed
    const contentEncoding = req.headers.get('content-encoding')
    if (contentEncoding !== 'gzip') {
      return new Response('Request body is not gzip compressed', {
        status: 400,
      })
    }

    // Read the compressed body
    const compressedBody = await req.arrayBuffer()

    // Decompress the body
    const decompressedBody = gunzipSync(new Uint8Array(compressedBody))

    // Convert the decompressed body to a string
    const decompressedString = new TextDecoder().decode(decompressedBody)
    const data = JSON.parse(decompressedString)
    // Process the decompressed body as needed
    console.log(`Received: ${data.length} logs.`)

    return new Response('ok', {
      headers: { 'Content-Type': 'text/plain' },
    })
  } catch (error) {
    console.error('Error:', error)
    return new Response('Error processing request', { status: 500 })
  }
})
```

    </AccordionItem>

</Accordion>

## DataDog logs

Logs sent to DataDog have the name of the log source set on the `service` field of the event and the source set to `Supabase`. Supabase gzips Logs before sending them to DataDog.

The payload message is a JSON string of the raw log event, prefixed with the event timestamp.

To setup a DataDog log drain, generate a DataDog API key [from your organization settings](https://app.datadoghq.com/organization-settings/api-keys) and the location of your DataDog site.

<Accordion
  type="default"
  openBehaviour="multiple"
>
    <AccordionItem
      header="Walkthrough"
      id="walkthrough"
    >

    1. Generate an API Key in [the DataDog dashboard](https://app.datadoghq.com/organization-settings/api-keys)
    2. Create a log drain in [the Supabase dashboard](/dashboard/project/_/settings/log-drains)
    3. Watch for events in [the DataDog Logs page](https://app.datadoghq.com/logs)

    </AccordionItem>

    <AccordionItem
      header="Example destination configuration"
      id="cfg"
    >

    The following is an example [Grok parser](https://docs.datadoghq.com/service_management/events/pipelines_and_processors/grok_parser?tab=matchers) matcher for extracting the timestamp to a `date` field.

    ```
    %{date("yyyy-MM-dd'T'HH:mm:ss.SSSSSSZZ"):date}
    ```

    The following is an example [Grok parser](https://docs.datadoghq.com/service_management/events/pipelines_and_processors/grok_parser?tab=matchers) matcher for converting stringified JSON to structured JSON on the `json` field.

    ```
    %{data::json}
    ```

    The following is an example [Remapper](https://docs.datadoghq.com/service_management/events/pipelines_and_processors/remapper) for setting the log level.

    ```
    metadata.parsed.error_severity, metadata.level
    ```

    </AccordionItem>

</Accordion>

<Admonition type="note">

If you are interested in other log drains, upvote them [in this GitHub discussion](https://github.com/orgs/supabase/discussions/28324)

</Admonition>

## Loki

Logs sent to the Loki HTTP API are specifically formatted according to the HTTP API requirements.  [Read the official Loki HTTP API documentation](https://grafana.com/docs/loki/latest/reference/loki-http-api/#ingest-logs) for more details.

This means the following:

- Events are batched with a maximum of 250 events per request.
- The log source and product are used as stream labels.
- The `event_message` and `timestamp` fields are dropped from the events to avoid duplicate data.

You must configure Loki to accept **structured metadata**, and you are advised to increase the default maximum number of structured metadata fields to at least 500 to accommodate large log event payloads of different products.

## Pricing

For a detailed breakdown of how charges are calculated, refer to [Manage Log Drain usage](/docs/guides/platform/manage-your-usage/log-drains).
